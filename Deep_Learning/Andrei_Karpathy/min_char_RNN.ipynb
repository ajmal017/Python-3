{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 8018 characters, 74 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "#data = open('party_nagy_lajos.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  #for t in xrange(len(inputs)):\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " vžťlGxGĂB-€.D±âO:TEˇ¶–ťrSAk‰HnNˇC‰¶rGelVPBAKĂPnybłFVpaŤGOˇ­icztSCGhchGVDfšgofrrF.O,to–Pâ2ziKrş9±-šJşâĽCmGTJCr–Sd:hĂŤ:?JM3P‘grˇSpołsrclplJy–,±SbP±ĽPOkPb9¶,NĂoťS ĂĹuT©€cŤMV–cOFyNvpaˇš Di€rIgžĂ2bAkfşB±aumS?ş3Juˇ‘xeln–E:!doxFNĽLeâBâa?Ăâ Dcb,o0!a±oy lDĂUBbmŤabVkeĽhN€zg¶.ťsu€eK­jTžĽf–xL22Ťa9Cłm-fa –K–:p?3 \n",
      "----\n",
      "iter 0, loss: 107.601631\n",
      "iter 100, loss: 107.603669\n",
      "iter 200, loss: 105.646622\n",
      "iter 300, loss: 103.321062\n",
      "iter 400, loss: 100.652285\n",
      "----\n",
      " e,riĽtni aze harsvermzigyzen aalt aˇ. o zek Ă©k meĂˇĂły erety Ă ail Ăˇtee. Ăˇr azer aaktĂˇrĂ¶pĂ©t e,eggĂĽn,es vĂłt solt Ăˇloz Nokiiˇ haz sĂ©vĂł aĂ­loycnĂlA‘n ab Ănrit og senĂˇtor nĂˇn‘h hoaa vĂłn oiTyl teg at t iKamaltaza . ma, ĂłzkĂgyattezrngi zĂĹmelitt aog aĂˇrvaa sia, kesĹit aherĂˇlA a vet, nentg \n",
      "----\n",
      "iter 500, loss: 97.907478\n",
      "iter 600, loss: 95.051088\n",
      "iter 700, loss: 92.702639\n",
      "iter 800, loss: 90.184903\n",
      "iter 900, loss: 87.591917\n",
      "----\n",
      " ĂłsotĂˇls, telkez Ăˇsšlovelas szebĂ©s ar szĂ©gd±z, vĂˇpo hil alnegal anidon‘enla mĂ©tokemangĹbmetnensogĂˇt asati, sogy,ata izekĂˇn Cvnvorntsa hat kĂ©g s‘vĂłtĂ©kĂłtĂłras bĂ©vEtntiĂ©velezĂłt vezsrĂlaman artĂ­zi kĂ©Ĺ‘lervĂˇletlaztatttĂˇndozĂˇlzhut BĂ©rultĂ©sĂłtegy leĂ©viktĂłtosm jĂšgy ezt rentatvĂlhĂĽr \n",
      "----\n",
      "iter 1000, loss: 85.325803\n",
      "iter 1100, loss: 83.292347\n",
      "iter 1200, loss: 81.146532\n",
      "iter 1300, loss: 79.215578\n",
      "iter 1400, loss: 77.494932\n",
      "----\n",
      " Ă©rti, Fegjan, ma‘ aukt, nĂłtaˇtttelambantan eztal, ag a tak hsĂˇ ĹegnantĂˇpĹ‘ megyek ak zĂĽ! fokzak, peszser ĂĽ mĂ©ks, feg a nel lĂ©fĂ©lĂˇt, EzĂˇrag, FĂ©x. Acs mĂ©vve azĂˇskĂ©gy sekĹ‘szĹ‘g, megysikĂ©mrĂ©ke ekaraszĂ©ltar aĂĽrdĂ0gen, adtĂˇdvem meakzĂĽtil am a mĂˇlo‘regĂ¶ ny a Azlea vogzenĂˇt delĂ¶t.  \n",
      "----\n",
      "iter 1500, loss: 75.808244\n",
      "iter 1600, loss: 74.234941\n",
      "iter 1700, loss: 72.797935\n",
      "iter 1800, loss: 71.440747\n",
      "iter 1900, loss: 70.130258\n",
      "----\n",
      " nĂ©nt e kumĂł s. Veg, hakĂˇlka slĂˇkzen szon fetttisoskttetre besnk Eetegya ment, vjlĂˇlĂĽk fol Ă¶ns VĂ©tĂĽn alitosut szas BĂ©bja kere negareltafsitt azĹ± a megĂ©n hogyita : ment ertt a t a aob!, rae mĂ©ny retti. Debbme : pĂ­vĂłt fen. kĂ©non cĹ‘m kĂ¶z, sĂ©veg ajmigy  Ă©r Ă©nt, Hot,, cranzon. Megyenm \n",
      "----\n",
      "iter 2000, loss: 68.938198\n",
      "iter 2100, loss: 67.856910\n",
      "iter 2200, loss: 66.701364\n",
      "iter 2300, loss: 65.785278\n",
      "iter 2400, loss: 64.905831\n",
      "----\n",
      "  FĂˇmkototterucsĂ©n vĂ©t, hsivĂłn a mĂˇlta vĂˇlta lelzha Pbedel kĂˇn tevĂłn tĂł MĂ©ndfĂ©rkĂ©svĂˇd,, Sssumta Ă©rnepĂ©lzosz ezba hĹ±yivulvĂłs ĂĽnbel van. PĂłt esĂˇnli, Bszeli a az espĂştnyett eta, el veggĂˇs, a n: hogyarĂ©sl negevĂˇs azĂˇs, Ă­gy nĂˇrp es cĂł mag alidehoz al, azilak mindoktesz eznel, e \n",
      "----\n",
      "iter 2500, loss: 63.881160\n",
      "iter 2600, loss: 63.094607\n",
      "iter 2700, loss: 62.467428\n",
      "iter 2800, loss: 61.660853\n",
      "iter 2900, loss: 60.907708\n",
      "----\n",
      " Ăˇtta am, men, Amin nĂštdetneb De. Ofşel FetvĂ©i Ool vĂĽjĂˇt megy hĂˇrdet a mĂˇn mikt, megy cĂşny kerecs. ntyattĂ©s hen nerba konevpĂ©t JĂ©sSs neĂˇs EsziszivĂłt bĂigba. AezĂ©nĂ©ntkikvdłkĂ©gĂˇnde, Ăb-kire vĂ©tentĹcsĂłl ogy ocjĂˇhojizxe nem sottat, szererencak tedtanelĹ‘ yagyĂˇt szĂł magymlbazilorttok \n",
      "----\n",
      "iter 3000, loss: 60.357889\n",
      "iter 3100, loss: 59.795625\n",
      "iter 3200, loss: 59.092154\n",
      "iter 3300, loss: 58.568465\n",
      "iter 3400, loss: 58.102191\n",
      "----\n",
      " ajot vegyat mevahelĂˇtkideliszĂˇ megpagyar ami PĂˇs, ha elttatĂ! Ostivet,Ă©g bedalĂ©g henvejtĂ©sza a bajĂ¶st Ĺegyem et.Ĺ‘t munyĂˇreg, hšje vĂ©g a BĂ©lenpĂ©szelem degĂˇnszĂĽlotvĂˇ Bagyabba MĂˇ nede, ccakisĂłi legeszerelĂˇkesz oztajabaz al elegyezletr at, lega e szandĂˇn, meg magyom, mon vegana Szllet \n",
      "----\n",
      "iter 3500, loss: 57.520860\n",
      "iter 3600, loss: 57.062394\n",
      "iter 3700, loss: 56.653543\n",
      "iter 3800, loss: 56.120819\n",
      "iter 3900, loss: 55.794134\n",
      "----\n",
      " vĂ©zz ĂĽlĂˇnepze k hĂ­r, Jer ezt,, biilĂ©rĹ‘ teregĂˇnt il lĂˇrtoszeli a felyĂ¶ndt Nanta hĂˇnveszottel jegehefĹ‘ta ogy besztlĂłk a kefĂˇt, horent, SĂˇlija Mopdi, Neg esett a honyit a magyarĂˇkĂł. soruas tegyevĂˇt, Pandel a fagyars szerer eztĂsze nen, migdott a. DĂˇbĂ©jlekvaltanor. EgcĂˇl KĂ©gpenel al \n",
      "----\n",
      "iter 4000, loss: 55.446538\n",
      "iter 4100, loss: 54.951872\n",
      "iter 4200, loss: 54.592339\n",
      "iter 4300, loss: 54.367666\n",
      "iter 4400, loss: 53.953510\n",
      "----\n",
      " get, hogy a PĂˇbal, hogy tezĂ©n eordulerorek a sĂ¶tKba kolem vĂłt korramibak Nomiltt a tĂ©ztĂblĂ­g hba avalosztollĂˇsĂł nuhĂ©k! Hegy Ă­zattal nyarettu.ogy a, mogjar esksukelevĂˇlĂĽllĂ©t la mĂ©szal, ntapit kororĂˇt, ĂĽrbĂ©roronkite, Szszett lĂłt fe BesenĂˇk, horbĂˇr Sumpne megy nyazenk szlettezta nat \n",
      "----\n",
      "iter 4500, loss: 53.524827\n",
      "iter 4600, loss: 53.336575\n",
      "iter 4700, loss: 53.058795\n",
      "iter 4800, loss: 52.629734\n",
      "iter 4900, loss: 52.433795\n",
      "----\n",
      " hegyĂ¶ri Ă©lĂĽk dĹ‘dĂłg ki¶letl, vadt negvĂłt akat ba alĂ©riki kĂ­rĂ©k eg!zĂˇsmiltnogya a Megy e rejain: jĂ©ris tulna vĂ©lel a geg azomaikota, ogzĂˇrs, ankog hony, a Ă©gett es rĂşl terbat eĹ, mĂ©rregikĂşgagyar ak a tĂĽljer el a hogy Bepbem heny pĂłvĂˇlĂˇs as, ki kĂˇlunab megy csacses Esi ocssztejta  \n",
      "----\n",
      "iter 5000, loss: 52.192554\n",
      "iter 5100, loss: 51.815451\n",
      "iter 5200, loss: 51.602514\n",
      "iter 5300, loss: 51.402053\n",
      "iter 5400, loss: 51.029972\n",
      "----\n",
      " zĂ©kirkivĂ©zt tencs TĹ‘tĂłlzontot, sokl, Mont Ăšn, merte esyariĂ©n nen bulĂˇn a ĂŤgyszt ĂšgzĂˇnkĂˇtenymin I vavĂ©bb a ksal bĂ©gres, mag szamt korĂ­tĹ‘t Ă©rtont tefejĂ¶ll sĂ©rdĹ±m ha ĂˇtkĂ©n nernyevevĂşga a bovĂłt P szifon, holt, vołt megy hunykilĂ©d! Oszeredest, ottott hĂˇllĹ‘k peszĂ©m elĂˇnt, Ăşgy  \n",
      "----\n",
      "iter 5500, loss: 50.834272\n",
      "iter 5600, loss: 50.629180\n",
      "iter 5700, loss: 50.310546\n",
      "iter 5800, loss: 50.075883\n",
      "iter 5900, loss: 49.990681\n",
      "----\n",
      " t Meg ezketa fultun mincolnatik , man vĂˇlĂˇ-tĂˇjĂ¶zbajibz, hosztvil, halyakokta, hug ak, hotn a sĂ©lar ambettott, oh-nlek, kiĂˇgysKĂˇdjĂ©gĂˇn le vansĂˇjĂ¶ny szĂ©gegyollat ars!t pa szol kcott, menkĂłz Ă©szen, Iiik a magyapĂł.Egyara a somunbalyada, hogy nem a kops ecta, Ă©n mĂˇr a arokka a koharirkĂł \n",
      "----\n",
      "iter 6000, loss: 49.739935\n",
      "iter 6100, loss: 49.420195\n",
      "iter 6200, loss: 49.321421\n",
      "iter 6300, loss: 49.140464\n",
      "iter 6400, loss: 48.839305\n",
      "----\n",
      " oret alttĂ¶nzsas Ă©rakikĂł hĂ©lĂ©k polbamteszettet a hop a ortĂł mzem âšyĂˇr SippĹ‘t, minda. Adhabak HotĂˇnt henkĂşgyok szetvĂ©kit, magyitl, ejpoltteg csal niket fĂ©ze Ăšj!. Scˇmtt megyzerĂşk, odnem nerende, csott aid, mendok az enga. EjĂ©rtĂˇsa, herbekkĂş,  kszĂ©pĂˇ doftba er vĂ©szel tervon, hogyvĂ \n",
      "----\n",
      "iter 6500, loss: 48.702632\n",
      "iter 6600, loss: 48.537795\n",
      "iter 6700, loss: 48.237516\n",
      "iter 6800, loss: 48.141209\n",
      "iter 6900, loss: 47.973207\n",
      "----\n",
      " tad lidĂˇnkolĂˇt, be meg lulen Ăşr Eryonhol vĂłt kincn-k ezpiss auktĂĽsonkas kĂˇrjdĂł bĂł an. BĂşgyvrjarrok, oddĂłvĂĽncsaztĂˇdszalus hzivel vĂłp vercsakojti helolsetjĂ©r der es, piny hejszĂˇnt pĂ©m eĹ‘vĂ­t valvett a ca, nym nakionta kĂ­zbinak a¶ velĂˇtlosTt lĂ¶rnĂˇlvĂˇra az, monk le kĂˇs anannĂˇ, so \n",
      "----\n",
      "iter 7000, loss: 47.699139\n",
      "iter 7100, loss: 47.549424\n",
      "iter 7200, loss: 47.420335\n",
      "iter 7300, loss: 47.194849\n",
      "iter 7400, loss: 47.093735\n",
      "----\n",
      " ĽnyĂˇt ezt Sende a szil ta magyap mĂˇre a kolĂmivĂł -Ăˇm hajiit miggan IcstĂşk ke merĂşsondĂł megşn sztĂ©szt fĂ©retĹ‘t, oszt tettem el tok alt, om mendepĂˇnta, temiet, hagymĂ©g eltmatjis Kziskden megrtĂ©kobĂˇs szecâ€M-gyakoak eszĂ©ksakĂˇllizt Ă©rkĂˇr ardĂ©rnetta kelivett ot tehmigyin a merem ellett  \n",
      "----\n",
      "iter 7500, loss: 47.084703\n",
      "iter 7600, loss: 46.896720\n",
      "iter 7700, loss: 46.641475\n",
      "iter 7800, loss: 46.583808\n",
      "iter 7900, loss: 46.478071\n",
      "----\n",
      " gyszhĂˇlnenittĂˇs, Ă©rzes Ăšg vĂˇt a nem ne balakkĂ©s pincmit, a linsanal mĂˇn te hĂ­gĂ©nrel. Egy ps‘kauhmi tĂĽllvĂłta Ihok utvanlĂłt, azba vĂłt lĂ©g eez eibbuna, a ha, pinvany jĂł PĂ©riga, vitt kĂ©gogkani, fe-nhel a hĂ©riresza vagymisi ĂĽsvedbĂˇn, hĂˇ JĂłt verecjĂłz nes Ă©ltes. A-kĂˇtĂł. ey-Ĺ±a tag \n",
      "----\n",
      "iter 8000, loss: 46.179183\n",
      "iter 8100, loss: 46.092368\n",
      "iter 8200, loss: 46.000170\n",
      "iter 8300, loss: 45.736651\n",
      "iter 8400, loss: 45.661087\n",
      "----\n",
      " a deg. FĂłvĂˇn nden a kolĂˇrba a cs. AlĂ©gzĂˇKĂ©s kĂˇllettunt megĂ¶tĂˇr. HejettĂĽp i feltĂşt kori a Mogpa a, vellĂˇlĂˇt migy reĂşk Ăłriri, Ss folvardelĂˇna a KollĂˇszkusztett Ăˇsta Ăşt, begyit barapsĂ©gii mĂ­gi em vĂˇnakiĂˇlĂˇllĂ¶l, ar vĂˇg nirkiles fĹ±Ăˇba. Dec-ele kestek lek a BĂˇr el sĂ¶st fĹ‘vĂł \n",
      "----\n",
      "iter 8500, loss: 45.612588\n",
      "iter 8600, loss: 45.367475\n",
      "iter 8700, loss: 45.237349\n",
      "iter 8800, loss: 45.144556\n",
      "iter 8900, loss: 45.002574\n",
      "----\n",
      " ekĂˇm vĂłkĂ¶szvĂĽl, vol fĂ­gztĂ©r ezeppilatn anyĂˇs MĂˇrĂˇs kesp tĂ©krott a vĂˇlĂˇny sĂˇg kzĂłviba boshonyontĂł ha az Ăšr Ă©gyĂ¶r Ăšr hĂ­g vĂ©ges zĂşrbaikKellĂ©t cĹ‘tĂ©s lende alaksirĹ‘rbĂˇn, dez Natababa. Ak bagy resztta vĂ©t, hor Ă©svinel es kĂ¶zmutt a rĂşg, agyszĂˇszĂˇk, munkok e nes va! AfomĂˇnk \n",
      "----\n",
      "iter 9000, loss: 44.865180\n",
      "iter 9100, loss: 44.850871\n",
      "iter 9200, loss: 44.726686\n",
      "iter 9300, loss: 44.509101\n",
      "iter 9400, loss: 44.479249\n",
      "----\n",
      " egyriĂ©rtottaââTportetĂˇszta magĂˇbsze pzĂłtert ar megy ba, magyar, merte. V?hmejottany Mega azi al, nem vĂłlĂ¶csĂ¶. EgmenekivĂ©ctuk, ha me Kes minyaba, olĂˇttĂˇn tesztĂłn e s Najgans a baltagatszett hijĂˇkĂˇlĂˇkzĂˇgy Om vĂłvantĂ©s Ă©ges emevĂ©ztĂ¶l kĂ¶rkion-kiogyinkat, mer nĂ­gybĂ©g tĂĽl valott a c \n",
      "----\n",
      "iter 9500, loss: 44.417463\n",
      "iter 9600, loss: 44.156689\n",
      "iter 9700, loss: 44.088463\n",
      "iter 9800, loss: 44.007338\n",
      "iter 9900, loss: 43.785932\n",
      "----\n",
      " hont kĂ¶zogyarbol, Jor vusztĂ¶tt, ak mĂˇl, mergĂˇna TĂ©ztek a pa, nir iin. Etyer, mondĂˇbĂˇT. Nen JĂˇrbĂ©s kĂ¶tb ne mak lemettĂĽkolt ĂˇlĂˇma ha. Egyan a fĂ¶val KolltĂşlbĂ©n fa riki, Mik AzĂˇla N adval, lennez Ă‰stĂ¶tĂˇt a bĂMhmin meslcĂˇl Ă©pĂ¶n Ăşgy meg t tĂ¶ltvĂˇsĂ©ny pĂ¶lrettĂ­jĂ©he hogyr a itsĂˇ \n",
      "----\n",
      "iter 10000, loss: 43.699668\n",
      "iter 10100, loss: 43.690494\n",
      "iter 10200, loss: 43.468948\n",
      "iter 10300, loss: 43.332607\n",
      "iter 10400, loss: 43.274833\n",
      "----\n",
      "  AlhĂşb kĂşy AzĂşg Ă­ztebbĂˇ..EEttol, nidvĂˇk, onpĂˇs al, mint naga, kinĂłtĂˇlatt a vĂłta dodnall a, jagy vĂł.EzĂ¶tt, jĂłdelen-k es aunt segavĂłm it vĂłt az lelter kiika, amedevĂ©n a sĂˇljeviron, aksudtĂłta a t aremen al, akogak azredotta lega, magĂˇ gott, hĂ­n ectĂ¶t vint a tĂ­nett a legnen vali, Ă \n",
      "----\n",
      "iter 10500, loss: 43.147205\n",
      "iter 10600, loss: 43.098205\n",
      "iter 10700, loss: 43.134511\n",
      "iter 10800, loss: 43.030152\n",
      "iter 10900, loss: 42.851437\n",
      "----\n",
      " elezt szlĂ©n adt mandankĂ©z ezenta a sokdoveten egyĂĽlĂˇn heggybegagtirort, Man hunĂ¶ndtt regy folymĂ©jegit szegyreiti, megykji icbalaggak, hĂˇkĂşs tagapkolĂ©valgot, csanta a urman Elben al Ăˇri, acst csefonetteregymĂˇg pait, mejet alymĂ©sĹ‘ksak, Er nenkett, mĂˇl, nem gĂˇzi polĂˇta PĂˇn ezet, uta az \n",
      "----\n",
      "iter 11000, loss: 42.806061\n",
      "iter 11100, loss: 42.711878\n",
      "iter 11200, loss: 42.448387\n",
      "iter 11300, loss: 42.395283\n",
      "iter 11400, loss: 42.318763\n",
      "----\n",
      " atĂˇmszegyals amłĂˇm, mĂˇn  azVĂłt mervankĂˇbâ€ť AvhejbĂ©szikĂłt ElyettĂˇsp telvĂˇn, hogy PĂˇr giszejĂˇvĂł  Ă­gystĂ­s ez? pagyarĂłror. E‰relerifĂˇnĂĽlibaritemett azszĂł jegelet, folyont heggĹ± oklĂˇszt a 20gyanyĹ‘ al tĂ©rizĂĽndotta pegy-kon, szezĹ‘les folĂˇba, mmbenrkifĂ©nyirtâog a kĂ¶lt: az, mondul \n",
      "----\n",
      "iter 11500, loss: 42.118396\n",
      "iter 11600, loss: 42.093108\n",
      "iter 11700, loss: 42.066574\n",
      "iter 11800, loss: 41.870844\n",
      "iter 11900, loss: 41.817411\n",
      "----\n",
      " veler vĂ©tkeme, a KĂ¶vetettĹ‘ â€žliosztepi, a pelĂˇrdofti, meg il. Alotn, tĂłbandetjbe e emher, ha besĂˇbb a, mitt. PĂ©zhebe Ă¶lvi eszĂ©negĹ‘t Ă©nde len vegpĂ©nyel meg mers elllamatondĂĽlelett Ă©rte busĂˇbb sĂ©kvunbek ezĂ©t senbel Ăˇlrallit te e sĂłlvertunha oĂˇrott, ha colltbedott Jerket nel punca  \n",
      "----\n",
      "iter 12000, loss: 41.775017\n",
      "iter 12100, loss: 41.717958\n",
      "iter 12200, loss: 41.575830\n",
      "iter 12300, loss: 41.578729\n",
      "iter 12400, loss: 41.437797\n",
      "----\n",
      "  a SOttjel, vĂ©rerkĂ©laz, hony ne erĹ‘, netvĂłt vĂłt kiłmen, onyadĂłna honder akladvilĂˇndĂˇlĂˇn kĂ¶zjon, Sombel man meggyel, ca, Edibal, tĂ¶ltvĂ©k tĂłt Ăşzm ba, mok łett, abt. Hetbe rĂłtĂ¶bm aztebĹ‘ Ă©gĂ¶k voni kovijt destval, vĂˇbĂşs Keregetta betyĂˇnyfĂ¶z ekĹ‘, sel nilmetĂˇba, mibbinek szavĂłt, Ă \n",
      "----\n",
      "iter 12500, loss: 41.229741\n",
      "iter 12600, loss: 41.256079\n",
      "iter 12700, loss: 41.206375\n",
      "iter 12800, loss: 40.999890\n",
      "iter 12900, loss: 40.987715\n",
      "----\n",
      " mint a szĂˇl te hĂ­ntoncâjĂł Magyara, mĂ©s PĂˇndĹ± korĂˇn, tetenyekmit, adri ak a ak agy Ă©jĂˇr hely moret. akiclis Ă©nek. âIiten FĂ©gott a velta ez meg edĹ‘tek allakkik. AkkĂ©‰jeg-fofpejĹ‘t Ă©n odi a vesze a sĂˇszĂş. AkĂł tegartok, cs. A szĂ©jlgatt pe nek esszeri ext a mĂˇlifok, az Ă­pen: kĂłt LĂ©v \n",
      "----\n",
      "iter 13000, loss: 40.916432\n",
      "iter 13100, loss: 40.746160\n",
      "iter 13200, loss: 40.701341\n",
      "iter 13300, loss: 40.654849\n",
      "iter 13400, loss: 40.487105\n",
      "----\n",
      " ĂˇsztĂşlt, memurap, tent a sĂ©hes sĂ­rte be JĹ‘szek magy rĂˇtĂ¶rKott. NĂˇrdocka hojdvott, namzeba Ii, Tšrmift, huma, mez PĂł, mimmegzĂ©s HelendeltĂł V kotemelevĂłn, hezĂˇkĂłjirtĂĽl-pivĂłt! As mzevar e gos vĂłt, Ă©rmen Ăşt tĂ©vilĂłt tĂ©vni azae tĂˇlvĂˇkul. GyĂ©gse pĂ©l felyencsĂ©gy tĂ©gyĂˇrĂş. Meva–n \n",
      "----\n",
      "iter 13500, loss: 40.392787\n",
      "iter 13600, loss: 40.322718\n",
      "iter 13700, loss: 40.261277\n",
      "iter 13800, loss: 40.146290\n",
      "iter 13900, loss: 40.216078\n",
      "----\n",
      "  kokĂˇndĹ±szĂˇmza lĂˇtjlet bejte. Matt nikiutkĂ¶s Ip-vett a kulakĂˇnkĂş, kett meg nĂˇgdĂ©nduloĂˇm-? jeraztesikjĂ©z etjal ncsĂł manyan a lerisĂˇbbĂˇntobjott, hoz ot lidĂˇniulligakocsak, Ă©zenkereri a tĂ¶zcba ket vĂłt, eli valĂˇnĂˇs. NĂˇr! puk alĂˇr. De ĂştĂˇnĂˇr Ăˇllot legĂ­js tamotta szĂˇkĂ¶ttĹ‘ KĂ© \n",
      "----\n",
      "iter 14000, loss: 40.104189\n",
      "iter 14100, loss: 39.989850\n",
      "iter 14200, loss: 40.044456\n",
      "iter 14300, loss: 40.021660\n",
      "iter 14400, loss: 39.815326\n",
      "----\n",
      " irtĂˇtĂł ! Csustak sarakorot, hogy nĂ¶lereninkazom, a mogyar a sĂ¶szvalsĂˇrra abrigĂ©gag. AĂˇruttertos teszvĂłt, hesrĂ©kivĂłk el tilert feleszĂˇk kerbet, as Ă©hal a handemz a sziĂˇna, mejĂˇsuk mistves. Scs szjegĂ©pĂ¶kckĂł KĂˇszt. AzjĂłr sercszor alysvira karĂşjĂˇsaltak a jededenyun. Mett pĂłrokhobba \n",
      "----\n",
      "iter 14500, loss: 39.839935\n",
      "iter 14600, loss: 39.791299\n",
      "iter 14700, loss: 39.635131\n",
      "iter 14800, loss: 39.597681\n",
      "iter 14900, loss: 39.518918\n",
      "----\n",
      " k hubSz ĂĽntĂˇbb, hokbĂł magĂˇrt, a csirĂ©lifĹ‘vĂłs kesenyetttal. Magya cselctak Magyar a dĂˇbb lelanyogoszetti terebe tivĂ©ni tondĂł a sz bony spenĂ­IgagyerĹ‘envĂ©s keszval tĂ©g volt, hon kia dalladvĂ©gy ankĂ¶rĂ¶n, dofvĂł ke puin, BĂ¶l nagyasâ€žKĂ­tte egyĂˇttol meg s. VĹpzivĂˇny esĹ‘jrĂˇllĂˇl mondy \n",
      "----\n",
      "iter 15000, loss: 39.365426\n",
      "iter 15100, loss: 39.298943\n",
      "iter 15200, loss: 39.239791\n",
      "iter 15300, loss: 39.099442\n",
      "iter 15400, loss: 38.957426\n",
      "----\n",
      " ˇnet, nem IsovĂ©llokkĂˇs tormunka tĂ©zdett, hogy detrĹ‘sat jercsuk nem vĂł meggysztenter-Magyar a KĂ¶zszĂˇgĂˇk korekeret, mir a ViĂˇt. Baznekt tehmagy, hogy min agy kĂ¶zvunt sĂ­gint. De kĂˇn mĂ­g sĂ©nnyel-zelszab, hogy, amilort sĂˇg le tĂłgagov ta lagyortt ehutĂˇrt herĂ©rbĂĽnk megy nem vellos, rekbĂ \n",
      "----\n",
      "iter 15500, loss: 38.986029\n",
      "iter 15600, loss: 38.858722\n",
      "iter 15700, loss: 38.680548\n",
      "iter 15800, loss: 38.690371\n",
      "iter 15900, loss: 38.638321\n",
      "----\n",
      " gycsevĂˇhjĂşt a lĂˇnlott a Ă‰retĹ‘stĂˇs, hainkĂˇn a ±migĂłsikĂˇnyobal hogy fĂ©vefĹ‘i Ă©gĂ­d, hosz pibba ha acpĂˇn maga a suhsanyonk, aval vĂˇsutt, a, abĂ©szĂˇt, meg telener jogĂn. NĂˇtjak Ă©goloftĂłkilĂĽlhajon. KĂ­nenifba dok, mĂ­legyarkĂĽl di a kegkizĂł a pĂĽllondĂˇszlĂ©nyest, ivenvelt, nem vĂłtaga \n",
      "----\n",
      "iter 16000, loss: 38.418452\n",
      "iter 16100, loss: 38.379534\n",
      "iter 16200, loss: 38.377698\n",
      "iter 16300, loss: 38.223788\n",
      "iter 16400, loss: 38.220971\n",
      "----\n",
      " ariĂşk MagyarĂˇk azta sz e hohot ez Ă©voltĂˇk ĂĽrĂ­ttospifĂ©k lĂ©csĂˇlletl vĂłt momĂˇbb. AhŤgy t az Ăšr vĂ©g lĂ©neg vĂ©let. Al form, hejenca vette begal man meg. JĂ©rez. BĂ©vĂˇtĂ©nkiny sikĂˇnr, et csĂĽg edgotte. KĂ©g edeni cĹ‘meg, majdokkortĂł kehalĂˇhom, tĂ¶rendet, hĂˇllag ele megyĂ©kĂ©rĂĹ± a sĂˇli \n",
      "----\n",
      "iter 16500, loss: 38.255409\n",
      "iter 16600, loss: 38.152648\n",
      "iter 16700, loss: 38.134874\n",
      "iter 16800, loss: 38.114113\n",
      "iter 16900, loss: 38.042809\n",
      "----\n",
      " emes ettĂş regy nĂ©riik, emzĂˇr gellend, hĂˇr! AkpegĂˇk aljunacsĂşs! NĂ©k AkvĂ©tĂˇr vĂłt, vek kĂ¶zĂ¶n Ăˇg a tamanki szĂ©kuszĂˇs, Get vandat ĂˇnyolĂˇllykĂłt, lireltĂĽk a niker es alttont szĂĽnnalinkatok sĂˇr, meg Ăşlesdven tĂ¶n nek vĂłtrĂˇl Ăˇr! Alomum VĂˇleszeztĂˇs, am udĂˇga a mĂ­vĂˇn hĂł PĂˇr a fĹ \n",
      "----\n",
      "iter 17000, loss: 38.009982\n",
      "iter 17100, loss: 38.097573\n",
      "iter 17200, loss: 37.970936\n",
      "iter 17300, loss: 37.779598\n",
      "iter 17400, loss: 37.808413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8766c00231ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[1;31m# sample from the model now and then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msample_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'----\\n %s \\n----'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-1e329b397b64>\u001b[0m in \u001b[0;36msample\u001b[0;34m(h, seed_ix, n)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 500 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 300)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-6f556aaf36fa>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-6f556aaf36fa>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    assert s0 == s1, 'Error dims dont match: %s and %s.' % (`s0`, `s1`)\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# gradient checking\n",
    "from random import uniform\n",
    "def gradCheck(inputs, target, hprev):\n",
    "  global Wxh, Whh, Why, bh, by\n",
    "  num_checks, delta = 10, 1e-5\n",
    "  _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "  for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "    s0 = dparam.shape\n",
    "    s1 = param.shape\n",
    "    assert s0 == s1, 'Error dims dont match: %s and %s.' % (`s0`, `s1`)\n",
    "    print name\n",
    "    for i in xrange(num_checks):\n",
    "      ri = int(uniform(0,param.size))\n",
    "      # evaluate cost at [x + delta] and [x - delta]\n",
    "      old_val = param.flat[ri]\n",
    "      param.flat[ri] = old_val + delta\n",
    "      cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val - delta\n",
    "      cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val # reset old value for this parameter\n",
    "      # fetch both numerical and analytic gradient\n",
    "      grad_analytic = dparam.flat[ri]\n",
    "      grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "      print '%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error)\n",
    "      # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
